{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework 12**\n",
    "\n",
    "**Note 1**: You should do this in a PySpark notebook, not a Python 3 notebook. Execute the first 3 cells that are provided.\n",
    "\n",
    "**Note 2**: In the following exercises, keep the amount of data returned by Spark to the local notebook session to the minimum needed for that exercise. In other words, all the work should be done via distributed computing and not by returning a large collection that is then processed in regular Python.\n",
    "\n",
    "**Note 3**: To minimize waitinng times, do the exercises using the C. elegans genome. Data for the human genome at `/data/human/*fa` but since it takes a long time, running your code against the human genome is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>62</td><td></td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkContext available as 'sc'.\n",
      "HiveContext available as 'sqlContext'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change path when debugging is compete to work on human genome\n",
    "\n",
    "# fasta_path = '/data/human/*fa'\n",
    "fasta_path = '/data/c_elegans/*fa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 (50 points)**\n",
    "\n",
    "Write a program using `spark` to find 5 most common k-mers (shifting windows of length k) in the human genome. Ignore case when processing k-mers. You can work one line at a time - we will ignore k-mers that wrap around lines. You should write a function that takes a path to FASTA files and a value for k, and returns an key-value RDD of k-mer counts. Remember to strip comment lines that begin with '>' from the anlaysis.  \n",
    "\n",
    "Use k = 20.\n",
    "\n",
    "**Note**: The textFile method takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Please set this paramter to 60 - it will speed up processing.\n",
    "\n",
    "**Check**: Use the C. elegans genome at `/data/c_elegans/*fa`. You should get \n",
    "\n",
    "```\n",
    "[\n",
    "(u'ATATATATATATATATATAT', 2168), \n",
    "(u'TATATATATATATATATATA', 2142), \n",
    "(u'CTCTCTCTCTCTCTCTCTCT', 1337), \n",
    "(u'TCTCTCTCTCTCTCTCTCTC', 1327), \n",
    "(u'AGAGAGAGAGAGAGAGAGAG', 1007)\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_elegans = sc.textFile('/data/c_elegans/*fa',60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count(line,k):\n",
    "    num=len(line)\n",
    "    line=line.upper()\n",
    "    counter=[]\n",
    "    if line[0]==\">\":\n",
    "        return counter\n",
    "    else:\n",
    "        for j in range(0,num-k+1):\n",
    "            window=line[j:j+k]\n",
    "            counter.append(window)\n",
    "        return counter\n",
    "\n",
    "def k_mers(path,k):\n",
    "    dictionary={}\n",
    "    file_name=sc.textFile(path,60)\n",
    "    words = file_name.flatMap(lambda line: count(line,k))\n",
    "    words1 = words.map(lambda x: (x, 1))\n",
    "    counts = words1.reduceByKey(lambda a, b: a + b)\n",
    "    return(counts,words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_mers_20=k_mers('/data/c_elegans/*fa',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'ATATATATATATATATATAT', 2217), (u'TATATATATATATATATATA', 2184), (u'CTCTCTCTCTCTCTCTCTCT', 1373), (u'TCTCTCTCTCTCTCTCTCTC', 1361), (u'AGAGAGAGAGAGAGAGAGAG', 1033), (u'GAGAGAGAGAGAGAGAGAGA', 1017), (u'TGCCGATTTGCCGGAAATTT', 772), (u'TGTGTGTGTGTGTGTGTGTG', 771), (u'TTGCCGATTTGCCGGAAATT', 766), (u'GTGTGTGTGTGTGTGTGTGT', 748)]"
     ]
    }
   ],
   "source": [
    "k_mers_20[0].takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 (10 points)**\n",
    "\n",
    "Find all k-mers that are palindromes (i.e the sequence is the same when read back-to-front). How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "palindromes=k_mers_20[1].map(lambda x: x==x[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(False, 68516766), (True, 2816)]"
     ]
    }
   ],
   "source": [
    "pals1 = palindromes.map(lambda x: (x, 1))\n",
    "count_pals = pals1.reduceByKey(lambda a, b: a + b)\n",
    "count_pals.takeOrdered(2, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 (10 points)** \n",
    "\n",
    "As a simple QC measure, we can assume that the k-mers that have a count of only 1 are due to sequencing errors. Put all the k-mers with a count of 2 or more in a Spark DataFrame with two columns (sequence, count). Count how many rows in the DataFrame have counts between 5 and 10 (inclusive of both 5 and 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_df = sqlc.createDataFrame(k_mers_20[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                  _1| _2|\n",
      "+--------------------+---+\n",
      "|TTTTGAACACTATCAAAAAA|  1|\n",
      "|TTAAATATACAAAAACATTG|  1|\n",
      "|GGTAGGCAGGCATGAGGTAG|  1|\n",
      "+--------------------+---+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "spark_df.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            sequence|count|\n",
      "+--------------------+-----+\n",
      "|CTGAAAAAATCAGCTTTTAT|    5|\n",
      "|AAATGCGTTTTTTGAACTTA|    3|\n",
      "|TACATCTTCTTTTGTAAGAC|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "spark_df.registerTempTable('k_mers')\n",
    "q = sqlc.sql('select _1 as sequence,_2 as count from k_mers where _2 > 1')\n",
    "q.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Number_3_to_5|\n",
      "+-------------+\n",
      "|        62224|\n",
      "+-------------+"
     ]
    }
   ],
   "source": [
    "q2 = sqlc.sql('select COUNT(_1) as Number_3_to_5 FROM k_mers WHERE _2>4 AND _2 < 6')\n",
    "q2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercsie 4 (30 points)**\n",
    "\n",
    "Make a Markov transition matrix for any nucleotide ('A', 'C', 'T', 'G') to any other nucleotide. The (i,j) entry should indicate the probability of finding the jth nucleotide appearing immediaely after the ith nucleotide in the genome. For example, the entry (0, 2) shows the probability of finding a T immediately followng an A. The matrix should have shape (4,4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_mers_2=k_mers('/data/c_elegans/*fa',2)\n",
    "markov_list=k_mers_2[0].takeOrdered(17, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'TT', 13345793), (u'AA', 13345542), (u'AT', 8726699), (u'TA', 6254722), (u'GA', 6127569), (u'TC', 6124442), (u'TG', 6101784), (u'CA', 6101366), (u'CT', 4994901), (u'AG', 4990349), (u'AC', 4765404), (u'GT', 4760088), (u'CC', 3309602), (u'GG', 3288847), (u'GC', 3284945), (u'CG', 3079340)]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "markov_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_total=markov_list[1][1]+markov_list[2][1]+markov_list[9][1]+markov_list[10][1]\n",
    "C_total=markov_list[7][1]+markov_list[8][1]+markov_list[12][1]+markov_list[15][1]\n",
    "T_total=markov_list[0][1]+markov_list[3][1]+markov_list[6][1]+markov_list[5][1]\n",
    "G_total=markov_list[4][1]+markov_list[11][1]+markov_list[13][1]+markov_list[14][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Markov_Transition=np.zeros([4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Markov_Transition[0,0]=float(markov_list[1][1])/float(A_total)\n",
    "Markov_Transition[0,1]=float(markov_list[10][1])/float(A_total)\n",
    "Markov_Transition[0,2]=float(markov_list[2][1])/float(A_total)\n",
    "Markov_Transition[0,3]=float(markov_list[9][1])/float(A_total)\n",
    "\n",
    "Markov_Transition[1,0]=float(markov_list[7][1])/float(C_total)\n",
    "Markov_Transition[1,1]=float(markov_list[12][1])/float(C_total)\n",
    "Markov_Transition[1,2]=float(markov_list[8][1])/float(C_total)\n",
    "Markov_Transition[1,3]=float(markov_list[15][1])/float(C_total)\n",
    "\n",
    "Markov_Transition[2,0]=float(markov_list[3][1])/float(T_total)\n",
    "Markov_Transition[2,1]=float(markov_list[5][1])/float(T_total)\n",
    "Markov_Transition[2,2]=float(markov_list[0][1])/float(T_total)\n",
    "Markov_Transition[2,3]=float(markov_list[6][1])/float(T_total)\n",
    "\n",
    "Markov_Transition[3,0]=float(markov_list[4][1])/float(G_total)\n",
    "Markov_Transition[3,1]=float(markov_list[14][1])/float(G_total)\n",
    "Markov_Transition[3,2]=float(markov_list[11][1])/float(G_total)\n",
    "Markov_Transition[3,3]=float(markov_list[13][1])/float(G_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.41930201,  0.14972367,  0.27418313,  0.15679119],\n",
      "       [ 0.34894441,  0.18928009,  0.28566436,  0.17611113],\n",
      "       [ 0.19652411,  0.1924307 ,  0.41932641,  0.19171878],\n",
      "       [ 0.35091985,  0.18812557,  0.27260556,  0.18834903]])"
     ]
    }
   ],
   "source": [
    "Markov_Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 1.,  1.,  1.,  1.])"
     ]
    }
   ],
   "source": [
    "np.sum(Markov_Transition,axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
